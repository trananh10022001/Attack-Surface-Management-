import io
import DB_Connection
import requests
from bs4 import BeautifulSoup



# tech = input('nhap domain: ') # Tên công nghệ bạn muốn tìm kiếm
# tech = 'vmware'
tech = 'Amazon web Service'

url = f'https://vulmon.com/searchpage?q={tech}&sortby=byrelevance'
response = requests.get(url)
if(response.status_code == 200):
    soup = BeautifulSoup(response.content, 'html.parser')
    page_num = soup.find_all('div', class_='ui pagination menu')
    if not page_num:
        num = 2
    else:
        for pages in page_num:
            soup = BeautifulSoup(pages.text, 'html.parser')
            pages = soup.extract('a')
            a = str(pages).replace("NEXT »",'').replace(' ', '')
            pages = len(a)
            num = pages - 2


for count in range(1, num):
    # print(count)
    url = f'https://vulmon.com/searchpage?q={tech}&sortby=byrelevance&page={count}'
    response = requests.get(url)
    if (response.status_code == 200):
        doc = response.text
        file = io.open('vulmon.html', mode = 'w', encoding='utf8')
        file.write(doc)

        soup = BeautifulSoup(response.content, 'html.parser')
        text = ""
        text1 = soup.find_all('a', class_= 'header')
        list1=[]
        for i in text1 :
            if(str(i).__contains__("header item") == False):
                soup2 = BeautifulSoup(i.text,features='html.parser')
                title = soup2.extract('a')
                list1.append(title)



        list2 = []
        text2 = soup.find_all('div',class_='value')
        for i in text2:
            soup3 = BeautifulSoup(i.text, 'html.parser')
            cvss_point = soup3.extract('div')
            list2.append(cvss_point)


        list3 = []
        text3 = soup.find_all('div',class_='description')
        for i in text3:
            soup3 = BeautifulSoup(i.text, 'html.parser')
            Descript = soup3.extract('a')
            list3.append(Descript)


        list4=[]
        text4 = soup.find_all('div',class_='extra')
        for i in text4:
            soup4 = BeautifulSoup(i.text, 'html.parser')
            web_tech = soup4.extract('div')
            list4.append(web_tech)


        list5 =[]
        for i in text1:
            if (str(i).__contains__("header item") == False):
                cve = i.get('href')
                cve_link = 'https://vulmon.com'+cve.strip()
                print(list5)


        cursor = DB_Connection.cursor

        for i in range(0,len(list1)):
            query = "INSERT INTO `asm`.`cve`(`cve_id`,`cvss_point`,`descriptions`,`web_tech`,`link`) VALUES ( '" +str(list1[i]) + "','" + str(list2[i]) + "','" + str(list3[i]).replace("&lt;=","before version").replace("'","") + "','" +str(list4[i])+ "','" +str(list5[i])+"');"
            print(query)
            cursor.execute(query)
        DB_Connection.connection.commit()
    else:
        break
